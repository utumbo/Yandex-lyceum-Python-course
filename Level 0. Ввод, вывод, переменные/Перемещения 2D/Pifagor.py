import math

x1 = float(input())
y1 = float(input())
x2 = float(input())
y2 = float(input())

S = math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)

print(round(S, 5))

"""
Расстояние между двумя точками на плоскости, заданными координатами, можно вычислить по теореме Пифагора.
S=√(x2−x1)**2+(y2−y1)**2
апишите программу, вычисляющую расстояние между двумя точками.

Формат вывода
Выведите расстояние с точностью до 5 знаков после запятой.

"""